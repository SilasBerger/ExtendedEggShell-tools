<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000061">
<title confidence="0.772661">
Against the Mainstream in Bug Prediction
</title>
<author confidence="0.876709">
Haidar Osman
</author>
<affiliation confidence="0.892077">
Software Composition Group
University of Bern, Switzerland
</affiliation>
<email confidence="0.987574">
osman@inf.unibe.ch
</email>
<sectionHeader confidence="0.990368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995081611111111">
Bug prediction is a technique used to estimate the most
bug-prone entities in software systems. Bug prediction
approaches vary in many design options, such as de-
pendent variables, independent variables, and machine
learning models. Choosing the right combination of de-
sign options to build an effective bug predictor is hard.
Previous studies do not consider this complexity and
draw conclusions based on fewer-than-necessary experi-
ments.
We argue that each software project is unique from the
perspective of its development process. Consequently,
metrics and AI models perform differently on different
projects, in the context of bug prediction.
We confirm our hypothesis empirically by running differ-
ent bug predictors on different systems. We show that
no single bug prediction configuration works globally on
all projects and, thus, previous bug prediction findings
cannot generalize.
</bodyText>
<sectionHeader confidence="0.997598" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9811908125">
A bug predictor is an intelligent system (model) trained on data derived from software (metrics)
to make a prediction (number of bugs, bug proneness, etc.) about software entities (packages,
classes, files, methods, etc.).
Bug prediction helps developers focus their quality assurance efforts on the parts of the system
that are more likely to contain bugs. Bug prediction takes advantage of the fact that bugs are not
evenly distributed across the system but they rather tend to cluster [19]. The distribution of the
bugs of the bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files
[17]. An efficective bug predictor locates the highest number of bugs in the least amount of code.
Over the last two decades, bug prediction has been a hot topic for research in software engi-
neering and many approaches have been devised to build effective bug predictors. Among the
scientific findings, two are agreed upon the most: (i) different machine learning models do not dif-
fer in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics
at predicting bugs [16][10][15][20][1][9][5].
Copyright cÃÂ© by the paperÃ¢ÂÂs authors. Copying permitted for private and academic purposes.
Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sat-
tose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org
</bodyText>
<page confidence="0.909059">
1
</page>
<bodyText confidence="0.9432625">
However, these studies do not consider the complexity of building a bug predictor, a process
that has many design options to choose from:
</bodyText>
<listItem confidence="0.641774">
1. The prediction model (e.g., Neural Network, Support Vector Machines, Random Forest, Lin-
ear Regression).
2. The independent variables (i.e., the metrics used to train the model like source code metrics,
change metrics, etc.).
3. The dependent variable or the model output (e.g., bug proneness, number of bugs, bug
density).
4. The granularity of prediction (e.g., package, class, binary).
5. The evaluation method (e.g., accuracy measures, percentage of bugs in percentage of software
</listItem>
<bodyText confidence="0.88895575">
entities).
Most previous approaches vary one design option, which is the studied one, and fix all oth-
ers. This affects the generalizability of the findings because every option affects the others and,
consequently, the overall outcome, as shown in Figure 1.
</bodyText>
<figure confidence="0.989613608695652">
Level of Prediction
Binary
Package / Module
Class / File
Method
Line of Code
Dependent Variable
Class (buggy/non-buggy)
Bug Proneness
Number of Bugs
Bug Density
Prediction Model
Regression
Probability
Binary
Cache
Evaluation Method
Confusion Matrix
Statistical Correlation
Percentage of Bugs
Cost-Aware Evaluation
Independent Variables
Source Code Metrics
Version History Metrics
Organizational Metrics
OutputInput
has an impact
on the
of theis s
uit
ab
le f
or decides
decides
has an
impac
t
on the h
as
an
im
pa
ct
on
th
e
</figure>
<figureCaption confidence="0.916647">
Figure 1: The design aspects of Bug prediction. The diagram shows the effects aspects have on
each other.
</figureCaption>
<bodyText confidence="0.999935857142857">
We hypothesise that bug prediction findings are inherently non-generalizable. A bug prediction
configuration that works with one system may not work with another because software systems
have different teams, development methods, frameworks, and architectures. All these factors affect
the correlation between different metrics and software defects.
To confirm our hypothesis, we run an extended empirical study where we try different bug
prediction configurations on different systems. We show that no single configuration generalizes
to all our subject systems and every system has its own Ã¢ÂÂbestÃ¢ÂÂ bug prediction configuration.
</bodyText>
<sectionHeader confidence="0.984989" genericHeader="method">
2 Experimental Setup
</sectionHeader>
<bodyText confidence="0.904854571428571">
Dataset
We run the experiments on the Ã¢ÂÂbug prediction data setÃ¢ÂÂ provided by DÃ¢ÂÂAmbros et al. [3] to
serve as a benchmark for bug prediction studies. This data set contains software metrics on the
class level for five software systems (Eclipse JDT Core, Eclipse PDE UI, Equinox Framework,
Lucene, and Mylyn). Using this data set constrains the level of prediction to be on the class level.
We compare source code metrics and version history metrics (change metrics) as the independent
variables.
</bodyText>
<page confidence="0.964713">
2
</page>
<subsectionHeader confidence="0.960223">
Dependent Variable
</subsectionHeader>
<bodyText confidence="0.99675">
All bug-prediction approaches predict one of the following: (1) the classification of the software
entity (buggy or bug-free), (2) the number of bugs in the software entity, (3) the probability of
a software entity to contain bugs (bug proneness), (4) the bug-density of a software entity (bugs
per LOC), or (5) the set of software entities that will contain bugs in the near future (e.g., within
a month). In this study, we consider two dependent variables: number of bugs and classification.
</bodyText>
<subsectionHeader confidence="0.942061">
Evaluation Method
</subsectionHeader>
<bodyText confidence="0.995149083333334">
An effective bug predictor should locate the highest number of bugs in the least amount of code.
Recently, researchers have drawn the attention to this principle and proposed evaluation schemes
to measure the cost or effort of using a bug prediction model [13][1][9][11][8][18][2]. Cost-aware
evaluation schemes rely on the fact that a bug predictor should produce an ordered list of the
software entities, then they measure the maximum percentage of predicted faults in the top k%
of lines of code of a system. These schemes take the number of lines of code (LOC) as a proxy for
the effort of unit testing and code reviewing.
In this study, we use an evaluation scheme called cost-effectiveness (CE), proposed by Arisholm
et al. [1]. CE ranges between Ã¢ÂÂ1 and +1. The closer CE gets to +1, the more cost-effective the
bug predictor is. A value of CE around zero indicates that there is no gain in using the bug
predictor. Once CE goes below zero, it means that using the bug predictor costs more than not
using it.
</bodyText>
<subsectionHeader confidence="0.373674">
Machine Learning Model
</subsectionHeader>
<bodyText confidence="0.98067525">
For classification, we use RandomForest (RF), K-Nearest Neighbour (KNN), Support Vector Ma-
chine (SVM), and Neural Networks (NN). To predict the number of bugs (regression), we use
linear regression (LR), SVM, KNN, and NN. We used the Weka data mining tool [7] to build
these prediction models1.
</bodyText>
<subsubsectionHeader confidence="0.955034">
Procedure
</subsubsectionHeader>
<bodyText confidence="0.99097425">
For every configuration, we randomly split the data set into a training set (70%) and a test set
(30%) in a way that retains the ratio between buggy and non-buggy entities. Then we train the
prediction model on the training set and run it on the test set and calculate the CE of the bug
predictor. For each configuration, we repeat this process 30 times and take the mean CE.
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.9732748">
First, we compare the different machine learning models. To see if machine learning models
perform differently, we apply the analysis of variance, ANOVA, and the post-hoc analysis, TukeyÃ¢ÂÂs
HSD (honest significant difference), among the different machine learning models in classification
and regression. The tests were carried out at 0.95 confidence level. Only when the ANOVA test
is statistically significant, we carry out the post-hoc test. Otherwise, we only report the best
preforming model. Statistically significant results are in bold. As can be seen from the results in
Table 1, It is clear that different machine learning models actually perform differently. Also there
is not dominant model that stands out as the best model throughout the experiments.
Second, to compare the two different types of metrics, we compare the most performing model
using source code metrics and the most performing model using change metrics using the t-student
test at the 95% confidence level. We do the comparison in both classification and regression.
Table 2 shows the results of the test, where bold text indicates statistically significant results. It
can be deduced from the results that source code metrics are better than change metrics in some
projects and worse in others. No type of metrics is constantly the best for all projects in the
dataset.
</bodyText>
<footnote confidence="0.964474">
1We use the Weka default configuration values for the models
</footnote>
<page confidence="0.87991">
3
</page>
<tableCaption confidence="0.996586">
Table 1: This table shows the results of the analysis of variance, ANOVA, and the post-hoc
</tableCaption>
<table confidence="0.843063294117647">
analysis, TukeyÃ¢ÂÂs HSD (honest significant difference). Bold text indicates statistically significant
results at 95% confidence level.
Classification
Metrics JDT PDE Equinox Mylyn Lucene
Version
History
NN &amp;gt; KNN
NN &amp;gt; RF
NN Ã¢ÂÂ¥ SVM
SV M &amp;gt; KNN
SVM Ã¢ÂÂ¥ RF
SVM Ã¢ÂÂ¥ NN
SVM
SV M &amp;gt; KNN
SV M &amp;gt; RF
SVM Ã¢ÂÂ¥ NN
SV M &amp;gt; NN
SVM Ã¢ÂÂ¥ RF
SVM Ã¢ÂÂ¥ KNN
Source
Code
KNN &amp;gt; SV N
KNN &amp;gt; NN
KNN &amp;gt; RF
KNN &amp;gt; SV N
KNN Ã¢ÂÂ¥ NN
KNN Ã¢ÂÂ¥ RF
KNN
NN &amp;gt; SV M
NN Ã¢ÂÂ¥ KNN
NN Ã¢ÂÂ¥ RF
RF &amp;gt; NN
RF Ã¢ÂÂ¥ SVM
RF Ã¢ÂÂ¥ KNN
Regression
Metrics JDT PDE Equinox Mylyn Lucene
Version
History
SV M &amp;gt; NN
SV M &amp;gt; KNN
SVM Ã¢ÂÂ¥ LR
LR &amp;gt; NN
LR &amp;gt; KNN
LR Ã¢ÂÂ¥ SVM
LR &amp;gt; KNN
LR Ã¢ÂÂ¥ SVM
LR Ã¢ÂÂ¥ NN
SV M &amp;gt; KNN
SV M &amp;gt; NN
SVM Ã¢ÂÂ¥ LR
LR
Source
Code
KNN &amp;gt; SV N
KNN &amp;gt; NN
KNN &amp;gt; LR
KNN &amp;gt; SV N
KNN &amp;gt; NN
KNN Ã¢ÂÂ¥ LR
SV M &amp;gt; KNN
SV M &amp;gt; NN
SVM Ã¢ÂÂ¥ LR
SV M &amp;gt; KNN
SV M &amp;gt; NN
SVM Ã¢ÂÂ¥ LR
KNN &amp;gt; NN
KNN Ã¢ÂÂ¥ SV N
KNN Ã¢ÂÂ¥ LR
</table>
<tableCaption confidence="0.814225666666667">
Table 2: This tables shows the t-student test between the best performing model trained on
source code metrics (SM) and the best performing model trained on change metrics (CM). Bold
text indicates the statistically significant results at 95% confidence level.
</tableCaption>
<table confidence="0.916496">
JDT PDE Equinox Mylyn Lucene
Classification CM &amp;lt; SM CM &amp;lt; SM CM &amp;gt; SM CM &amp;gt; SM CM &amp;gt; SM
Regression CM Ã¢ÂÂ¥ SM CM Ã¢ÂÂ¤ SM CM Ã¢ÂÂ¤ SM CM &amp;gt; SM CM &amp;gt; SM
</table>
<page confidence="0.87519">
4
</page>
<tableCaption confidence="0.698838857142857">
Table 3: This tables shows the t-student test between the classifier (CLA) and the best regressor
(REG). The test is carried out at the 0.95 confidence level. Bold text indicates the statistically
significant results at 95% confidence level.
JDT PDE Equinox Mylyn Lucene
REG Ã¢ÂÂ¥ CLA REG Ã¢ÂÂ¥ CLA REG &amp;gt; CLA REG Ã¢ÂÂ¥ CLA REG Ã¢ÂÂ¥ CLA
Table 4: The most cost-effective bug prediction configuration for each system and the correspond-
ing mean CE.
</tableCaption>
<table confidence="0.966626285714286">
Subject Independent Variables Prediction Model Dependent Variable Mean
System (Metrics) (Output) CE
Eclipse JDT Core Change Metrics SVM Number of Bugs 0.356
Eclipse PDE UI Source Code Metrics KNN Number of Bugs 0.246
Equinox Source Code Metrics SVM Number of Bugs 0.429
Mylyn Change Metrics SVM Number of Bugs 0.484
Lucene Change Metrics LR Number of Bugs 0.588
</table>
<bodyText confidence="0.997794888888889">
Third, we compare the the two types of response variables (classification vs regression) by
comparing the best performing model from each using also the t-test statistical test at the 95%
confidence level. Table 3 shows that the comparisons are in favour of regression all projects in our
dataset but with statistical significance only in case of Equinox. This means that treating bug
prediction as a regression problem is more cost effective than classification.
Finally, we compare configurations with the highest CE for the five projects in the data set. In
Table 4, we report the highest mean CE and the configuration of the bug predictor behind. The
results show that there is no global configuration of settings that suits all projects.
To summarize the results of the experiments, we make the following observations:
</bodyText>
<listItem confidence="0.8471662">
1. Different machine learning models actually perform differently in predicting bugs and there
is no dominant model that stands out as the best for all projects.
2. There is no general rule about which metrics are better at predicting bugs.
3. The configurations of the most cost-effective bug predictor vary from one project to another.
4. The cost effectiveness of bug prediction is different from one system to another.
</listItem>
<sectionHeader confidence="0.993269" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.990536">
Building a software bug predictor is a complex process with many interleaving design choices.
In the bug prediction literature, researchers have overlooked this complexity, suggesting general-
izability where none is warranted. We argue that bug prediction studies cannot be generalized
because software systems are different. Among the five subject systems we have, no type of metrics
stands out as the best and no machine learning algorithm prevails when building for building a
cost-effective bug predictor. This indicates a need for more research to revisit literature findings
while taking bug prediction complexity into account. Also, the direction of research in this do-
main should change from looking for silver bullets to looking for project-specific most effective
configurations.
</bodyText>
<page confidence="0.677712">
5
</page>
<sectionHeader confidence="0.969448" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999741512195122">
[1] E. Arisholm, L. C. Briand, and E. B. Johannessen. A systematic and comprehensive investi-
gation of methods to build and evaluate fault prediction models. J. Syst. Softw., 83(1):2Ã¢ÂÂ17,
Jan. 2010.
[2] G. Canfora, A. De Lucia, M. Di Penta, R. Oliveto, A. Panichella, and S. Panichella. Multi-
objective cross-project defect prediction. In Software Testing, Verification and Validation
(ICST), 2013 IEEE Sixth International Conference on, pages 252Ã¢ÂÂ261, Mar. 2013.
[3] M. DÃ¢ÂÂAmbros, M. Lanza, and R. Robbes. An extensive comparison of bug prediction ap-
proaches. In Proceedings of MSR 2010 (7th IEEE Working Conference on Mining Software
Repositories), pages 31Ã¢ÂÂ40. IEEE CS Press, 2010.
[4] K. O. Elish and M. O. Elish. Predicting defect-prone software modules using support vector
machines. Journal of Systems and Software, 81(5):649Ã¢ÂÂ660, 2008.
[5] E. Giger, M. DÃ¢ÂÂAmbros, M. Pinzger, and H. C. Gall. Method-level bug prediction. In
Proceedings of the ACM-IEEE international symposium on Empirical software engineering
and measurement, pages 171Ã¢ÂÂ180. ACM, 2012.
[6] L. Guo, Y. Ma, B. Cukic, and H. Singh. Robust prediction of fault-proneness by random
forests. In Software Reliability Engineering, 2004. ISSRE 2004. 15th International Symposium
on, pages 417Ã¢ÂÂ428. IEEE, 2004.
[7] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The weka
data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10Ã¢ÂÂ18, 2009.
[8] H. Hata, O. Mizuno, and T. Kikuno. Bug prediction based on fine-grained module histories.
In Proceedings of the 34th International Conference on Software Engineering, ICSE Ã¢ÂÂ12, pages
200Ã¢ÂÂ210, Piscataway, NJ, USA, 2012. IEEE Press.
[9] Y. Kamei, S. Matsumoto, A. Monden, K.-i. Matsumoto, B. Adams, and A. Hassan. Revisiting
common bug prediction findings using effort-aware models. In Software Maintenance (ICSM),
2010 IEEE International Conference on, pages 1Ã¢ÂÂ10, Sept. 2010.
[10] S. Kim, T. Zimmermann, E. J. W. Jr., and A. Zeller. Predicting faults from cached history.
In ICSE Ã¢ÂÂ07: Proceedings of the 29th international conference on Software Engineering, pages
489Ã¢ÂÂ498, Washington, DC, USA, 2007. IEEE Computer Society.
[11] K. Kobayashi, A. Matsuo, K. Inoue, Y. Hayase, M. Kamimura, and T. Yoshino. ImpactScale:
Quantifying change impact to predict faults in large software systems. In Proceedings of the
2011 27th IEEE International Conference on Software Maintenance, ICSM Ã¢ÂÂ11, pages 43Ã¢ÂÂ52,
Washington, DC, USA, 2011. IEEE Computer Society.
[12] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch. Benchmarking classification models for
software defect prediction: A proposed framework and novel findings. IEEE Trans. Softw.
Eng., 34(4):485Ã¢ÂÂ496, July 2008.
[13] T. Mende and R. Koschke. Revisiting the evaluation of defect prediction models. In Pro-
ceedings of the 5th International Conference on Predictor Models in Software Engineering,
PROMISE Ã¢ÂÂ09, pages 7:1Ã¢ÂÂ7:10, New York, NY, USA, 2009. ACM.
[14] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. Bener. Defect prediction from
static code features: Current results, limitations, new approaches. Automated Software Engg.,
17(4):375Ã¢ÂÂ407, Dec. 2010.
</reference>
<page confidence="0.893292">
6
</page>
<reference confidence="0.999327210526316">
[15] R. Moser, W. Pedrycz, and G. Succi. A comparative analysis of the efficiency of change metrics
and static code attributes for defect prediction. In Proceedings of the 30th International
Conference on Software Engineering, ICSE Ã¢ÂÂ08, pages 181Ã¢ÂÂ190, New York, NY, USA, 2008.
ACM.
[16] T. Ostrand, E. Weyuker, and R. Bell. Predicting the location and number of faults in large
software systems. Software Engineering, IEEE Transactions on, 31(4):340Ã¢ÂÂ355, Apr. 2005.
[17] T. J. Ostrand, E. J. Weyuker, and R. M. Bell. Where the bugs are. In ACM SIGSOFT
Software Engineering Notes, volume 29, pages 86Ã¢ÂÂ96. ACM, 2004.
[18] F. Rahman, D. Posnett, and P. Devanbu. Recalling the Ã¢ÂÂimprecisionÃ¢ÂÂ of cross-project defect
prediction. In In the 20th ACM SIGSOFT FSE. ACM, 2012.
[19] S. A. Sherer. Software fault prediction. Journal of Systems and Software, 29(2):97Ã¢ÂÂ105, 1995.
[20] E. J. Weyuker, T. J. Ostrand, and R. M. Bell. Do too many cooks spoil the broth? using the
number of developers to enhance defect prediction models. Empirical Softw. Engg., 13(5):539Ã¢ÂÂ
559, Oct. 2008.
[21] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy. Cross-project defect
prediction: A large scale experiment on data vs. domain vs. process. In Proceedings of the the
7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT
Symposium on The Foundations of Software Engineering, ESEC/FSE Ã¢ÂÂ09, pages 91Ã¢ÂÂ100, New
York, NY, USA, 2009. ACM.
</reference>
<page confidence="0.997299">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823902">
<title confidence="0.999863">Against the Mainstream in Bug Prediction</title>
<author confidence="0.996145">Haidar Osman</author>
<affiliation confidence="0.940831">Software Composition Group University of Bern, Switzerland</affiliation>
<email confidence="0.962221">osman@inf.unibe.ch</email>
<abstract confidence="0.998818210526316">Bug prediction is a technique used to estimate the most bug-prone entities in software systems. Bug prediction approaches vary in many design options, such as dependent variables, independent variables, and machine learning models. Choosing the right combination of design options to build an effective bug predictor is hard. Previous studies do not consider this complexity and draw conclusions based on fewer-than-necessary experiments. We argue that each software project is unique from the perspective of its development process. Consequently, metrics and AI models perform differently on different projects, in the context of bug prediction. We confirm our hypothesis empirically by running different bug predictors on different systems. We show that no single bug prediction configuration works globally on all projects and, thus, previous bug prediction findings cannot generalize.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Arisholm</author>
<author>L C Briand</author>
<author>E B Johannessen</author>
</authors>
<title>A systematic and comprehensive investigation of methods to build and evaluate fault prediction models.</title>
<date>2010</date>
<journal>J. Syst. Softw.,</journal>
<volume>83</volume>
<issue>1</issue>
<contexts>
<context position="2184" citStr="[1]" startWordPosition="334" endWordPosition="334">ollow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector Machines, Random Forest, Linear Regression). 2. The independent variables (i.e., the metrics used to train the mode</context>
<context position="5933" citStr="[1]" startWordPosition="911" endWordPosition="911">, (3) the probability of a software entity to contain bugs (bug proneness), (4) the bug-density of a software entity (bugs per LOC), or (5) the set of software entities that will contain bugs in the near future (e.g., within a month). In this study, we consider two dependent variables: number of bugs and classification. Evaluation Method An effective bug predictor should locate the highest number of bugs in the least amount of code. Recently, researchers have drawn the attention to this principle and proposed evaluation schemes to measure the cost or effort of using a bug prediction model [13][1][9][11][8][18][2]. Cost-aware evaluation schemes rely on the fact that a bug predictor should produce an ordered list of the software entities, then they measure the maximum percentage of predicted faults in the top k% of lines of code of a system. These schemes take the number of lines of code (LOC) as a proxy for the effort of unit testing and code reviewing. In this study, we use an evaluation scheme called cost-effectiveness (CE), proposed by Arisholm et al. [1]. CE ranges between Ã¢1 and +1. The closer CE gets to +1, the more cost-effective the bug predictor is. A value of CE around zero</context>
</contexts>
<marker>[1]</marker>
<rawString>E. Arisholm, L. C. Briand, and E. B. Johannessen. A systematic and comprehensive investigation of methods to build and evaluate fault prediction models. J. Syst. Softw., 83(1):2Ã¢ÂÂ17, Jan. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Canfora</author>
<author>A De Lucia</author>
<author>M Di Penta</author>
<author>R Oliveto</author>
<author>A Panichella</author>
<author>S Panichella</author>
</authors>
<title>Multiobjective cross-project defect prediction.</title>
<date>2013</date>
<booktitle>In Software Testing, Verification and Validation (ICST), 2013 IEEE Sixth International Conference on,</booktitle>
<pages>252--261</pages>
<contexts>
<context position="5950" citStr="[2]" startWordPosition="911" endWordPosition="911">lity of a software entity to contain bugs (bug proneness), (4) the bug-density of a software entity (bugs per LOC), or (5) the set of software entities that will contain bugs in the near future (e.g., within a month). In this study, we consider two dependent variables: number of bugs and classification. Evaluation Method An effective bug predictor should locate the highest number of bugs in the least amount of code. Recently, researchers have drawn the attention to this principle and proposed evaluation schemes to measure the cost or effort of using a bug prediction model [13][1][9][11][8][18][2]. Cost-aware evaluation schemes rely on the fact that a bug predictor should produce an ordered list of the software entities, then they measure the maximum percentage of predicted faults in the top k% of lines of code of a system. These schemes take the number of lines of code (LOC) as a proxy for the effort of unit testing and code reviewing. In this study, we use an evaluation scheme called cost-effectiveness (CE), proposed by Arisholm et al. [1]. CE ranges between Ã¢1 and +1. The closer CE gets to +1, the more cost-effective the bug predictor is. A value of CE around zero indicates that t</context>
</contexts>
<marker>[2]</marker>
<rawString>G. Canfora, A. De Lucia, M. Di Penta, R. Oliveto, A. Panichella, and S. Panichella. Multiobjective cross-project defect prediction. In Software Testing, Verification and Validation (ICST), 2013 IEEE Sixth International Conference on, pages 252Ã¢ÂÂ261, Mar. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M DÃ¢Ambros</author>
<author>M Lanza</author>
<author>R Robbes</author>
</authors>
<title>An extensive comparison of bug prediction approaches.</title>
<date>2010</date>
<booktitle>In Proceedings of MSR 2010 (7th IEEE Working Conference on Mining Software Repositories),</booktitle>
<pages>31--40</pages>
<publisher>IEEE CS Press,</publisher>
<contexts>
<context position="4738" citStr="[3]" startWordPosition="718" endWordPosition="718">h another because software systems have different teams, development methods, frameworks, and architectures. All these factors affect the correlation between different metrics and software defects. To confirm our hypothesis, we run an extended empirical study where we try different bug prediction configurations on different systems. We show that no single configuration generalizes to all our subject systems and every system has its own Ã¢bestÃ¢ bug prediction configuration. 2 Experimental Setup Dataset We run the experiments on the Ã¢bug prediction data setÃ¢ provided by DÃ¢Ambros et al. [3] to serve as a benchmark for bug prediction studies. This data set contains software metrics on the class level for five software systems (Eclipse JDT Core, Eclipse PDE UI, Equinox Framework, Lucene, and Mylyn). Using this data set constrains the level of prediction to be on the class level. We compare source code metrics and version history metrics (change metrics) as the independent variables. 2 Dependent Variable All bug-prediction approaches predict one of the following: (1) the classification of the software entity (buggy or bug-free), (2) the number of bugs in the software entity, (3) th</context>
</contexts>
<marker>[3]</marker>
<rawString>M. DÃ¢ÂÂAmbros, M. Lanza, and R. Robbes. An extensive comparison of bug prediction approaches. In Proceedings of MSR 2010 (7th IEEE Working Conference on Mining Software Repositories), pages 31Ã¢ÂÂ40. IEEE CS Press, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K O Elish</author>
<author>M O Elish</author>
</authors>
<title>Predicting defect-prone software modules using support vector machines.</title>
<date>2008</date>
<journal>Journal of Systems and Software,</journal>
<volume>81</volume>
<issue>5</issue>
<contexts>
<context position="2077" citStr="[4]" startWordPosition="320" endWordPosition="320">tributed across the system but they rather tend to cluster [19]. The distribution of the bugs of the bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector Machines</context>
</contexts>
<marker>[4]</marker>
<rawString>K. O. Elish and M. O. Elish. Predicting defect-prone software modules using support vector machines. Journal of Systems and Software, 81(5):649Ã¢ÂÂ660, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Giger</author>
<author>M DÃ¢Ambros</author>
<author>M Pinzger</author>
<author>H C Gall</author>
</authors>
<title>Method-level bug prediction.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACM-IEEE international symposium on Empirical software engineering and measurement,</booktitle>
<pages>171--180</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="2084" citStr="[5]" startWordPosition="320" endWordPosition="320">d across the system but they rather tend to cluster [19]. The distribution of the bugs of the bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector Machines, Rando</context>
</contexts>
<marker>[5]</marker>
<rawString>E. Giger, M. DÃ¢ÂÂAmbros, M. Pinzger, and H. C. Gall. Method-level bug prediction. In Proceedings of the ACM-IEEE international symposium on Empirical software engineering and measurement, pages 171Ã¢ÂÂ180. ACM, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Guo</author>
<author>Y Ma</author>
<author>B Cukic</author>
<author>H Singh</author>
</authors>
<title>Robust prediction of fault-proneness by random forests.</title>
<date>2004</date>
<booktitle>In Software Reliability Engineering,</booktitle>
<pages>417--428</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="2070" citStr="[6]" startWordPosition="320" endWordPosition="320">nly distributed across the system but they rather tend to cluster [19]. The distribution of the bugs of the bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector M</context>
</contexts>
<marker>[6]</marker>
<rawString>L. Guo, Y. Ma, B. Cukic, and H. Singh. Robust prediction of fault-proneness by random forests. In Software Reliability Engineering, 2004. ISSRE 2004. 15th International Symposium on, pages 417Ã¢ÂÂ428. IEEE, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="6969" citStr="[7]" startWordPosition="1089" endWordPosition="1089">(CE), proposed by Arisholm et al. [1]. CE ranges between Ã¢1 and +1. The closer CE gets to +1, the more cost-effective the bug predictor is. A value of CE around zero indicates that there is no gain in using the bug predictor. Once CE goes below zero, it means that using the bug predictor costs more than not using it. Machine Learning Model For classification, we use RandomForest (RF), K-Nearest Neighbour (KNN), Support Vector Machine (SVM), and Neural Networks (NN). To predict the number of bugs (regression), we use linear regression (LR), SVM, KNN, and NN. We used the Weka data mining tool [7] to build these prediction models1. Procedure For every configuration, we randomly split the data set into a training set (70%) and a test set (30%) in a way that retains the ratio between buggy and non-buggy entities. Then we train the prediction model on the training set and run it on the test set and calculate the CE of the bug predictor. For each configuration, we repeat this process 30 times and take the mean CE. 3 Results First, we compare the different machine learning models. To see if machine learning models perform differently, we apply the analysis of variance, ANOVA, and the post-h</context>
</contexts>
<marker>[7]</marker>
<rawString>M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The weka data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10Ã¢ÂÂ18, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hata</author>
<author>O Mizuno</author>
<author>T Kikuno</author>
</authors>
<title>Bug prediction based on fine-grained module histories.</title>
<date>2012</date>
<booktitle>In Proceedings of the 34th International Conference on Software Engineering, ICSE Ã¢12,</booktitle>
<pages>200--210</pages>
<publisher>IEEE Press.</publisher>
<location>Piscataway, NJ, USA,</location>
<contexts>
<context position="5943" citStr="[8]" startWordPosition="911" endWordPosition="911">probability of a software entity to contain bugs (bug proneness), (4) the bug-density of a software entity (bugs per LOC), or (5) the set of software entities that will contain bugs in the near future (e.g., within a month). In this study, we consider two dependent variables: number of bugs and classification. Evaluation Method An effective bug predictor should locate the highest number of bugs in the least amount of code. Recently, researchers have drawn the attention to this principle and proposed evaluation schemes to measure the cost or effort of using a bug prediction model [13][1][9][11][8][18][2]. Cost-aware evaluation schemes rely on the fact that a bug predictor should produce an ordered list of the software entities, then they measure the maximum percentage of predicted faults in the top k% of lines of code of a system. These schemes take the number of lines of code (LOC) as a proxy for the effort of unit testing and code reviewing. In this study, we use an evaluation scheme called cost-effectiveness (CE), proposed by Arisholm et al. [1]. CE ranges between Ã¢1 and +1. The closer CE gets to +1, the more cost-effective the bug predictor is. A value of CE around zero indicates</context>
</contexts>
<marker>[8]</marker>
<rawString>H. Hata, O. Mizuno, and T. Kikuno. Bug prediction based on fine-grained module histories. In Proceedings of the 34th International Conference on Software Engineering, ICSE Ã¢ÂÂ12, pages 200Ã¢ÂÂ210, Piscataway, NJ, USA, 2012. IEEE Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kamei</author>
<author>S Matsumoto</author>
<author>A Monden</author>
<author>K-i Matsumoto</author>
<author>B Adams</author>
<author>A Hassan</author>
</authors>
<title>Revisiting common bug prediction findings using effort-aware models.</title>
<date>2010</date>
<booktitle>In Software Maintenance (ICSM), 2010 IEEE International Conference on,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="2187" citStr="[9]" startWordPosition="334" endWordPosition="334">ow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector Machines, Random Forest, Linear Regression). 2. The independent variables (i.e., the metrics used to train the model l</context>
<context position="5936" citStr="[9]" startWordPosition="911" endWordPosition="911">3) the probability of a software entity to contain bugs (bug proneness), (4) the bug-density of a software entity (bugs per LOC), or (5) the set of software entities that will contain bugs in the near future (e.g., within a month). In this study, we consider two dependent variables: number of bugs and classification. Evaluation Method An effective bug predictor should locate the highest number of bugs in the least amount of code. Recently, researchers have drawn the attention to this principle and proposed evaluation schemes to measure the cost or effort of using a bug prediction model [13][1][9][11][8][18][2]. Cost-aware evaluation schemes rely on the fact that a bug predictor should produce an ordered list of the software entities, then they measure the maximum percentage of predicted faults in the top k% of lines of code of a system. These schemes take the number of lines of code (LOC) as a proxy for the effort of unit testing and code reviewing. In this study, we use an evaluation scheme called cost-effectiveness (CE), proposed by Arisholm et al. [1]. CE ranges between Ã¢1 and +1. The closer CE gets to +1, the more cost-effective the bug predictor is. A value of CE around zero in</context>
</contexts>
<marker>[9]</marker>
<rawString>Y. Kamei, S. Matsumoto, A. Monden, K.-i. Matsumoto, B. Adams, and A. Hassan. Revisiting common bug prediction findings using effort-aware models. In Software Maintenance (ICSM), 2010 IEEE International Conference on, pages 1Ã¢ÂÂ10, Sept. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>T Zimmermann</author>
<author>E J W Jr</author>
<author>A Zeller</author>
</authors>
<title>Predicting faults from cached history.</title>
<date>2007</date>
<booktitle>In ICSE Ã¢07: Proceedings of the 29th international conference on Software Engineering,</booktitle>
<pages>489--498</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA,</location>
<contexts>
<context position="2173" citStr="[10]" startWordPosition="334" endWordPosition="334">f the bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector Machines, Random Forest, Linear Regression). 2. The independent variables (i.e., the metrics used to tra</context>
</contexts>
<marker>[10]</marker>
<rawString>S. Kim, T. Zimmermann, E. J. W. Jr., and A. Zeller. Predicting faults from cached history. In ICSE Ã¢ÂÂ07: Proceedings of the 29th international conference on Software Engineering, pages 489Ã¢ÂÂ498, Washington, DC, USA, 2007. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kobayashi</author>
<author>A Matsuo</author>
<author>K Inoue</author>
<author>Y Hayase</author>
<author>M Kamimura</author>
<author>T Yoshino</author>
</authors>
<title>ImpactScale: Quantifying change impact to predict faults in large software systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 27th IEEE International Conference on Software Maintenance, ICSM Ã¢11,</booktitle>
<pages>43--52</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA,</location>
<contexts>
<context position="5940" citStr="[11]" startWordPosition="911" endWordPosition="911">the probability of a software entity to contain bugs (bug proneness), (4) the bug-density of a software entity (bugs per LOC), or (5) the set of software entities that will contain bugs in the near future (e.g., within a month). In this study, we consider two dependent variables: number of bugs and classification. Evaluation Method An effective bug predictor should locate the highest number of bugs in the least amount of code. Recently, researchers have drawn the attention to this principle and proposed evaluation schemes to measure the cost or effort of using a bug prediction model [13][1][9][11][8][18][2]. Cost-aware evaluation schemes rely on the fact that a bug predictor should produce an ordered list of the software entities, then they measure the maximum percentage of predicted faults in the top k% of lines of code of a system. These schemes take the number of lines of code (LOC) as a proxy for the effort of unit testing and code reviewing. In this study, we use an evaluation scheme called cost-effectiveness (CE), proposed by Arisholm et al. [1]. CE ranges between Ã¢1 and +1. The closer CE gets to +1, the more cost-effective the bug predictor is. A value of CE around zero indica</context>
</contexts>
<marker>[11]</marker>
<rawString>K. Kobayashi, A. Matsuo, K. Inoue, Y. Hayase, M. Kamimura, and T. Yoshino. ImpactScale: Quantifying change impact to predict faults in large software systems. In Proceedings of the 2011 27th IEEE International Conference on Software Maintenance, ICSM Ã¢ÂÂ11, pages 43Ã¢ÂÂ52, Washington, DC, USA, 2011. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lessmann</author>
<author>B Baesens</author>
<author>C Mues</author>
<author>S Pietsch</author>
</authors>
<title>Benchmarking classification models for software defect prediction: A proposed framework and novel findings.</title>
<date>2008</date>
<journal>IEEE Trans. Softw. Eng.,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="2074" citStr="[12]" startWordPosition="320" endWordPosition="320"> distributed across the system but they rather tend to cluster [19]. The distribution of the bugs of the bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector Machi</context>
</contexts>
<marker>[12]</marker>
<rawString>S. Lessmann, B. Baesens, C. Mues, and S. Pietsch. Benchmarking classification models for software defect prediction: A proposed framework and novel findings. IEEE Trans. Softw. Eng., 34(4):485Ã¢ÂÂ496, July 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mende</author>
<author>R Koschke</author>
</authors>
<title>Revisiting the evaluation of defect prediction models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 5th International Conference on Predictor Models in Software Engineering, PROMISE Ã¢09,</booktitle>
<pages>7--1</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="5930" citStr="[13]" startWordPosition="911" endWordPosition="911">tity, (3) the probability of a software entity to contain bugs (bug proneness), (4) the bug-density of a software entity (bugs per LOC), or (5) the set of software entities that will contain bugs in the near future (e.g., within a month). In this study, we consider two dependent variables: number of bugs and classification. Evaluation Method An effective bug predictor should locate the highest number of bugs in the least amount of code. Recently, researchers have drawn the attention to this principle and proposed evaluation schemes to measure the cost or effort of using a bug prediction model [13][1][9][11][8][18][2]. Cost-aware evaluation schemes rely on the fact that a bug predictor should produce an ordered list of the software entities, then they measure the maximum percentage of predicted faults in the top k% of lines of code of a system. These schemes take the number of lines of code (LOC) as a proxy for the effort of unit testing and code reviewing. In this study, we use an evaluation scheme called cost-effectiveness (CE), proposed by Arisholm et al. [1]. CE ranges between Ã¢1 and +1. The closer CE gets to +1, the more cost-effective the bug predictor is. A value of CE around z</context>
</contexts>
<marker>[13]</marker>
<rawString>T. Mende and R. Koschke. Revisiting the evaluation of defect prediction models. In Proceedings of the 5th International Conference on Predictor Models in Software Engineering, PROMISE Ã¢ÂÂ09, pages 7:1Ã¢ÂÂ7:10, New York, NY, USA, 2009. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Menzies</author>
<author>Z Milton</author>
<author>B Turhan</author>
<author>B Cukic</author>
<author>Y Jiang</author>
<author>A Bener</author>
</authors>
<title>Defect prediction from static code features: Current results, limitations, new approaches.</title>
<date>2010</date>
<journal>Automated Software Engg.,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="2081" citStr="[14]" startWordPosition="320" endWordPosition="320">buted across the system but they rather tend to cluster [19]. The distribution of the bugs of the bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector Machines, Ra</context>
</contexts>
<marker>[14]</marker>
<rawString>T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. Bener. Defect prediction from static code features: Current results, limitations, new approaches. Automated Software Engg., 17(4):375Ã¢ÂÂ407, Dec. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moser</author>
<author>W Pedrycz</author>
<author>G Succi</author>
</authors>
<title>A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th International Conference on Software Engineering, ICSE Ã¢08,</booktitle>
<pages>181--190</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="2177" citStr="[15]" startWordPosition="334" endWordPosition="334">e bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector Machines, Random Forest, Linear Regression). 2. The independent variables (i.e., the metrics used to train t</context>
</contexts>
<marker>[15]</marker>
<rawString>R. Moser, W. Pedrycz, and G. Succi. A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction. In Proceedings of the 30th International Conference on Software Engineering, ICSE Ã¢ÂÂ08, pages 181Ã¢ÂÂ190, New York, NY, USA, 2008. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ostrand</author>
<author>E Weyuker</author>
<author>R Bell</author>
</authors>
<title>Predicting the location and number of faults in large software systems.</title>
<date>2005</date>
<journal>Software Engineering, IEEE Transactions on,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="2169" citStr="[16]" startWordPosition="334" endWordPosition="334">gs of the bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector Machines, Random Forest, Linear Regression). 2. The independent variables (i.e., the metrics used to</context>
</contexts>
<marker>[16]</marker>
<rawString>T. Ostrand, E. Weyuker, and R. Bell. Predicting the location and number of faults in large software systems. Software Engineering, IEEE Transactions on, 31(4):340Ã¢ÂÂ355, Apr. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Ostrand</author>
<author>E J Weyuker</author>
<author>R M Bell</author>
</authors>
<title>Where the bugs are.</title>
<date>2004</date>
<journal>In ACM SIGSOFT Software Engineering Notes,</journal>
<volume>29</volume>
<pages>86--96</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="1667" citStr="[17]" startWordPosition="252" endWordPosition="252"> an intelligent system (model) trained on data derived from software (metrics) to make a prediction (number of bugs, bug proneness, etc.) about software entities (packages, classes, files, methods, etc.). Bug prediction helps developers focus their quality assurance efforts on the parts of the system that are more likely to contain bugs. Bug prediction takes advantage of the fact that bugs are not evenly distributed across the system but they rather tend to cluster [19]. The distribution of the bugs of the bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and a</context>
</contexts>
<marker>[17]</marker>
<rawString>T. J. Ostrand, E. J. Weyuker, and R. M. Bell. Where the bugs are. In ACM SIGSOFT Software Engineering Notes, volume 29, pages 86Ã¢ÂÂ96. ACM, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rahman</author>
<author>D Posnett</author>
<author>P Devanbu</author>
</authors>
<title>Recalling the Ã¢imprecisionÃ¢ of cross-project defect prediction.</title>
<date>2012</date>
<booktitle>In In the 20th ACM SIGSOFT FSE. ACM,</booktitle>
<contexts>
<context position="5947" citStr="[18]" startWordPosition="911" endWordPosition="911">bability of a software entity to contain bugs (bug proneness), (4) the bug-density of a software entity (bugs per LOC), or (5) the set of software entities that will contain bugs in the near future (e.g., within a month). In this study, we consider two dependent variables: number of bugs and classification. Evaluation Method An effective bug predictor should locate the highest number of bugs in the least amount of code. Recently, researchers have drawn the attention to this principle and proposed evaluation schemes to measure the cost or effort of using a bug prediction model [13][1][9][11][8][18][2]. Cost-aware evaluation schemes rely on the fact that a bug predictor should produce an ordered list of the software entities, then they measure the maximum percentage of predicted faults in the top k% of lines of code of a system. These schemes take the number of lines of code (LOC) as a proxy for the effort of unit testing and code reviewing. In this study, we use an evaluation scheme called cost-effectiveness (CE), proposed by Arisholm et al. [1]. CE ranges between Ã¢1 and +1. The closer CE gets to +1, the more cost-effective the bug predictor is. A value of CE around zero indicates tha</context>
</contexts>
<marker>[18]</marker>
<rawString>F. Rahman, D. Posnett, and P. Devanbu. Recalling the Ã¢ÂÂimprecisionÃ¢ÂÂ of cross-project defect prediction. In In the 20th ACM SIGSOFT FSE. ACM, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Sherer</author>
</authors>
<title>Software fault prediction.</title>
<date>1995</date>
<journal>Journal of Systems and Software,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="1537" citStr="[19]" startWordPosition="227" endWordPosition="227">on works globally on all projects and, thus, previous bug prediction findings cannot generalize. 1 Introduction A bug predictor is an intelligent system (model) trained on data derived from software (metrics) to make a prediction (number of bugs, bug proneness, etc.) about software entities (packages, classes, files, methods, etc.). Bug prediction helps developers focus their quality assurance efforts on the parts of the system that are more likely to contain bugs. Bug prediction takes advantage of the fact that bugs are not evenly distributed across the system but they rather tend to cluster [19]. The distribution of the bugs of the bugs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code</context>
</contexts>
<marker>[19]</marker>
<rawString>S. A. Sherer. Software fault prediction. Journal of Systems and Software, 29(2):97Ã¢ÂÂ105, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Weyuker</author>
<author>T J Ostrand</author>
<author>R M Bell</author>
</authors>
<title>Do too many cooks spoil the broth? using the number of developers to enhance defect prediction models. Empirical Softw.</title>
<date>2008</date>
<journal>Engg.,</journal>
<volume>13</volume>
<issue>5</issue>
<pages>559</pages>
<contexts>
<context position="2181" citStr="[20]" startWordPosition="334" endWordPosition="334">gs follow the Pareto principle, i.e., 80% of the bugs are located in 20% of the files [17]. An efficective bug predictor locates the highest number of bugs in the least amount of code. Over the last two decades, bug prediction has been a hot topic for research in software engineering and many approaches have been devised to build effective bug predictors. Among the scientific findings, two are agreed upon the most: (i) different machine learning models do not differ in predicting bugs [6][12][4][14][5], and (ii) change metrics are better than source code metrics at predicting bugs [16][10][15][20][1][9][5]. Copyright cÃÂ© by the paperÃ¢s authors. Copying permitted for private and academic purposes. Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution SATToSE2016 (sattose.org), Bergen, Norway, 11-13 July 2016, published at http://ceur-ws.org 1 However, these studies do not consider the complexity of building a bug predictor, a process that has many design options to choose from: 1. The prediction model (e.g., Neural Network, Support Vector Machines, Random Forest, Linear Regression). 2. The independent variables (i.e., the metrics used to train the m</context>
</contexts>
<marker>[20]</marker>
<rawString>E. J. Weyuker, T. J. Ostrand, and R. M. Bell. Do too many cooks spoil the broth? using the number of developers to enhance defect prediction models. Empirical Softw. Engg., 13(5):539Ã¢ÂÂ 559, Oct. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zimmermann</author>
<author>N Nagappan</author>
<author>H Gall</author>
<author>E Giger</author>
<author>B Murphy</author>
</authors>
<title>Cross-project defect prediction: A large scale experiment on data vs. domain vs. process.</title>
<date>2009</date>
<booktitle>In Proceedings of the the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering, ESEC/FSE Ã¢09,</booktitle>
<pages>91--100</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<marker>[21]</marker>
<rawString>T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy. Cross-project defect prediction: A large scale experiment on data vs. domain vs. process. In Proceedings of the the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering, ESEC/FSE Ã¢ÂÂ09, pages 91Ã¢ÂÂ100, New York, NY, USA, 2009. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>