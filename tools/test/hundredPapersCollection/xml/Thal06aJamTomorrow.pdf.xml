<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.344366666666667">
Jam Tomorrow: Collaborative Music Generation in Croquet Using OpenAL
Florian Thalmann
thalmann@students.unibe.ch
Markus Gaelli
gaelli@iam.unibe.ch
Institute of Computer Science and Applied Mathematics, University of Bern
Neubrückstrasse 10, CH-3012 Bern, Switzerland
Abstract
1 We propose a music generation software that allows
</figure>
<bodyText confidence="0.997435625">
large numbers of users to collaborate. In a virtual world,
groups of users generate music simultaneously at different
places in a room. This can be realized using OpenAL sound
sources. The generated musical pieces have to be modi-
fiable while they are playing and all collaborating users
should immediately see and hear the results of such mod-
ifications. We are testing these concepts within Croquet by
implementing a software called Jam Tomorrow.
</bodyText>
<sectionHeader confidence="0.99329" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.988627684210526">
“It’s very good jam” said the Queen.
“Well, I don’t want any today, at any rate.”
“You couldn’t have it if you did want it,” the
Queen said. “The rule is jam tomorrow and jam
yesterday but never jam today.”
“It must come sometimes to ‘jam today,”’Alice
objected.
“No it can’t,” said the Queen. “It’s jam every
other day; today isn’t any other day, you know.”
“I don’t understand you,” said Alice. “It’s dread-
fully confusing.”
Lewis Carroll, Through the Looking Glass, 1871.
Even though the idea of collaborative music generation
on computer networks is almost thirty years old, today there
are only a few projects in this domain [15]. Improvisation
over the web has always been technically constrained, due
to traffic limitations and synchronicity problems, but is now
possible using MIDI [3] and even using compressed au-
dio [14]. There are also projects about collaborative com-
</bodyText>
<footnote confidence="0.798118">
1Fourth International Conference on Creating, Connecting and Collab-
orating through Computing (C5’06) pages 73–78
</footnote>
<bodyText confidence="0.9894255">
posing [15] [13] [1] or collaborative open source record-
ing [12]. People using such compositional or improvisa-
tional software are restricted to working on one musical
piece at a time. Our idea is to develop a software that lets
users collaborate in generating multiple musical pieces si-
multaneously within a modern virtual world. People using
this software will have compositional and improvisational
possibilities.
In this paper, we start by presenting our conceptual ideas.
In section 3 we explain the technology we use for our explo-
rative project, Jam Tomorrow. We continue in section 4 by
giving you a more detailed description of the musical con-
cept and the architecture of Jam Tomorrow. In the same sec-
tion we describe the major problems we encountered during
implementation. Finally, we conclude by discussing future
work.
</bodyText>
<sectionHeader confidence="0.398392" genericHeader="keywords">
2. Concept: Multiple Musicians in One Virtual
Room
</sectionHeader>
<bodyText confidence="0.946957173913043">
Our idea is to build an application that consists of sev-
eral similar musical editors (graphical user interfaces) dis-
tributed within a virtual room. Each of these editors is
linked to a musical piece and can be used as an interface
for modifying it. The corresponding musical piece is heard
from a sound source located at the editor’s position.
The virtual room is visited by multiple users, which can
move around freely between the editors. Depending on a
user’s distance from an editor, volume and panning of this
editor’s music changes. So, a user can find the editor of
the music he is interested in, simply by following sound
sources.
Every user can use any editor. If somebody modifies a
musical piece, this has to be immediately audible to all users
near its editor. Of course, this constrains the musical possi-
bilities a lot. For our ideas of such a flexible and collabora-
tive music, see section 4.2.
With our concept, we can have a large number of tunes in
one huge musical world. Users in this world can collaborate
in groups or work alone, modify an existing tune or start a
new one. It is certainly very interesting to walk through
this world and hear the editors play the music reflecting the
common likes of the users.
</bodyText>
<listItem confidence="0.864753">
3. Technology: Croquet Project and OpenAL
</listItem>
<bodyText confidence="0.996235740740741">
OpenAL is an API that “allows a programmer to po-
sition audio sources in a three-dimensional space around
a listener, producing reasonable fading and panning
for each source so that the environment seems three-
dimensional” [6]. This is exactly what we need for the
placement of our musical pieces within the room. Unfor-
tunately, at the time, OpenAL does not support MIDI yet,
so we are restricted to sampled sound [2].
“Croquet is a combination of open source computer
software and network architecture that supports deep col-
laboration and resource sharing among large numbers of
users” [16] and it is implemented in Squeak. Although
Croquet is still being developed, it seems to be the perfect
framework for the integration of our application.
A Croquet environment is divided into spaces, in which
users can move around using their avatar. Unlike in client-
server applications, each participant in a Croquet network
session, has an independent image holding the entire appli-
cation on his computer. At the moment, for establishing a
connection between Croquet users, all images have to be in
the same state and contain the same program code. To keep
the images synchronized, every message sent to a Croquet
object in a space, is replicated in the other participants im-
age. To replicate a message manually, the #meta message
has to be sent. For instance, ‘self update’ becomes ‘self
meta update’. For understanding our architecture, it is im-
portant to know these principles.
</bodyText>
<subsectionHeader confidence="0.888803">
3.1. Croquet’s OpenAL Interface and its
Performance
</subsectionHeader>
<bodyText confidence="0.993803673076923">
An implementation of OpenAL is already integrated in
Croquet and the sampled sound file formats wav, aif and
mp3 are supported. During the development of Jam To-
morrow, we explored the possibilities of Croquet’s OpenAL
interface. Before talking about the implementation of our
project, we would like to discuss the performance limita-
tions we have.
The basic OpenAL Croquet class we use is TSound. A
TSound holds a SoundBuffer containing a sampled sound
and manages the OpenAL buffers and sources needed for
playing. It also understands common control messages like
#play, #pause and #stop. One can assign a TFrame (a Cro-
quet object with a position) to a TSound, so that when
this TSound is played, its OpenAL source is located at the
TFrame’s position in the Croquet space.
TSound or similar classes like OpenALStreamingSound,
are currently the only way to use OpenAL in Croquet. If we
want to play a sound, we have to copy its sound buffer to a
TSound object’s sound buffer and then start streaming the
TSound (fill the OpenAL buffers). We have to do this once,
if we plan to use the same sound repeatedly. But if we want
to play different sounds, we have to copy the buffer and
stream the sound for every new sound.
If we do this for short sounds a user has saved or gen-
erates locally, performance is fairly good. But performance
gets worse, as soon as sounds have to be sent over the net-
work. For example with the current voice chatting imple-
mentation (TVoiceRecorder), when somebody records au-
dio, it takes too much time for the sound to reach the other
participant’s computers. For musicians playing simultane-
ously, latency time needs to be very close to zero.
Due to these problems, we are limited to using locally
saved or generated sounds. Improvisation over network
using microphones is therefore not yet possible. To inte-
grate realtime MIDI functions using TSounds would result
in an even more unsatisfying performance. We have ex-
perimented with Squeak’s FMSynth, but the translation to
TSounds is too slow for complex MIDI messages. How-
ever, it could be possible to build a simple MIDI applica-
tion, where every note is pregenerated as TSound.
We hope, that OpenAL will soon support MIDI for this
would increase our musical flexibility dramatically. There
are software products that translate MIDI to audio [17], but
not yet in realtime.
At the moment, Croquet cannot be used for virtual band
playing in realtime yet. This is why we chose to build a first
application that does not need to be strongly synchronous.
In our concept it is important, that every participant hears
exactly the samemusic, but not necessarily at the same time.
For example, it is possible, that the user who presses the
’play’-button, hears the music a bit earlier than his fellow
users.
</bodyText>
<sectionHeader confidence="0.337924" genericHeader="method">
4. Implementation: Jam Tomorrow
</sectionHeader>
<bodyText confidence="0.987516714285714">
For experimentation with Croquet and OpenAL, we have
built a simple prototype called Jam Tomorrow. It consists of
a number of editors (GUI windows), each of them linked to
a sound player. Users can add different types of tracks to
a player, modify and play them. We also integrated a func-
tion to record samples and add them to the player. But this
function cannot be used yet, due to its poor performance.
</bodyText>
<subsectionHeader confidence="0.824174">
4.1. A Sample Scenario
</subsectionHeader>
<bodyText confidence="0.992410818181818">
As an example, we have a Croquet space with three ed-
itors and four user avatars in it (see Fig. 1). Jane and John
collaborate modifying the musical piece in editor 1. We
will see later, what exactly they are doing. They both hear
the music they are producing in editor 1, but they also hear
Max’s music in editor 2 (Jane a bit louder than John). Max
prefers to make his own music, but if somebody else wants
to join him, he has to be happy with it. Min enjoys the sit-
uation doing nothing and listening to the different changing
musical pieces. The music in editor 3 is not playing at the
time, so she hears editor 1 on her right hand side and editor
</bodyText>
<figure confidence="0.978430181818182">
2 on her left hand side.
Jane
editor 2
John
Max
Min
Croquet Space
Croquet Avatars
editor 1
editor 3
Croquet TWindow
</figure>
<figureCaption confidence="0.999404">
Figure 1. A space with avatars and editors.
</figureCaption>
<bodyText confidence="0.997299684210526">
The important thing now, is how Jane and John are able
to collaborate: Jane decides to make the music sound more
interesting. She selects a sound file that fits the musical
context. So she adds this file using editor 1. She presses
the generate button and the sound file is transformed into a
loop sample (see next section). Meanwhile, John sees her
pointer moving, so he is able to watch everything she is
doing. When the sample is ready, John hears it, integrated
in the music.
But John, does not like the way the sample sounds. He
thinks it is too loud and too simple. So first he turns down
the sample’s volume and then decides to choose another al-
gorithm to generate the sample. He prefers experimental
music and therefore lets a random fragmentation algorithm
randomize the sample. The first recalculation does not fit
the music at all, so Jane chooses to press the generate but-
ton again. Now, they are lucky: the new sample is so beau-
tiful that Min decides to join them. She has some incredible
ideas that will change John and Jane’s lives.
</bodyText>
<subsectionHeader confidence="0.678111">
4.2. Musical Concept
</subsectionHeader>
<bodyText confidence="0.941328333333333">
Even though, for our explorative prototype, we devel-
oped a rather simple musical concept, this is just a sugges-
tion that may be replaced or expanded in later development.
However, this concept is simple to implement and fulfills
the following needs of a flexible musical form.
Approaching one of the editors, a user should immedi-
</bodyText>
<listItem confidence="0.925163333333333">
ately hear what the music generated there is about. Then,
when somebody modifies this music, the results should be
heard immediately. Additionally, the music should be mod-
ifiable while it is playing. We thought of two musical forms
guaranteeing such flexibility, while being simple enough.
• The first form has become very popular since the elec-
</listItem>
<bodyText confidence="0.9793864">
trification of music: the loop form. It lets us gener-
ate “infinite” musical pieces that always continue play-
ing (looping), even while they are being edited. So,
all modifications can be audible immediately, because
there are no long formal structures to wait for.
</bodyText>
<listItem confidence="0.833174">
• The second form is total improvisation. Users partic-
</listItem>
<bodyText confidence="0.988185545454546">
ipate live using a microphone (or a MIDI interface).
However, with the current Croquet implementation, it
is impossible to integrate an improvisational function
for microphone or MIDI interfaces, due to synchronic-
ity problems.
Jam Tomorrow is based on the loop form, although we
extended it for experimental reasons. We also integrated the
possibility to generate Musinum tracks. Musinum [4] [8] is
an algorithmic music generation software based on fractals.
See section 4.4 for information about the implementation of
our musical concept.
</bodyText>
<subsectionHeader confidence="0.479621">
4.3. The Jam Tomorrow Architecture
</subsectionHeader>
<bodyText confidence="0.939341761904762">
During development, we encountered several problems,
mainly with the Croquet concept. Our goal was to use un-
modified Croquet and Squeak parts, but we found out that
what we were planning to implement was not possible with-
out changing some methods in the Croquet framework.
What we have now, is an architecture that can be seen as
an application of the MVC (Model-View-Controller) pat-
tern [11] in Croquet. We have a model consisting of the
player and its tracks. It is the responsibility of this model
to keep the corresponding models of the other images in
the network synchronized (see Fig. 2). Each model has one
view by default, which is updated automatically. The views
in the different images only know their own models, so they
are separated from each other.
This architecture with separated views does not fit the
Croquet concept perfectly. However, this is not noticeable
for users, because the views stay synchronized through their
model 2
model 1
model 3
editor 1
</bodyText>
<figure confidence="0.976958285714286">
event
synchronization
editor 2 editor 3
update update
computer 1
computer 2 computer 3
update
</figure>
<figureCaption confidence="0.9689865">
Figure 2. Jam Tomorrow’s architecture in ac-
tion.
</figureCaption>
<listItem confidence="0.690446333333333">
models. Additionally, we have three advantages using such
an architecture.
• Croquet classes managing their own synchronization,
</listItem>
<bodyText confidence="0.906760076923077">
i.e. having methods containing meta sends (TSound,
for example), can be used unchanged (see section 4.5).
• We are able to program nondeterministic functions,
where the model, which receives the message from its
view, first calculates the nondeterministic part (Fig. 3).
Then, the first model updates the other models with
the result, and each model continues calculation on
its own. Finally, each view is updated by its model.
This concept is used in FragmentedSample (see sec-
tion 4.4.1).
time
editor 1 model 1 editor 2 model 2 editor 3 model 3
other models are updated with result
</bodyText>
<figureCaption confidence="0.728713666666667">
method is called
nondeterministic part is calculated
result is processed
views are updated
Figure 3. A sequence diagram for nondeter-
ministic functions.
</figureCaption>
<listItem confidence="0.9644595">
• Separated functions can be attached directly to the GUI
(musical editor). A GUI button, for example, can have
</listItem>
<bodyText confidence="0.981355225">
a function that only has a local effect. Usually in
Croquet, for such functionality, an overlay button is
added to the global Croquet user interface. However,
our method keeps the musical editor’s UI more com-
pact and simple. We used this concept for the sample
recording function. If a user presses the record button,
recording starts on his computer only.
4.4. The Model Classes
Our actual version of Jam Tomorrow can be seen as an
“intelligent” loop device. We have a JamTomorrowPlayer,
which holds a collection of JamTomorrowTracks. There
are two kinds of tracks: LoopSamples and MusinumTracks,
both of which have a specified playing probability. Users
can define how often a loop should be played.
LoopSamples are based on sampled sounds imported
from a specified sound file. When imported, these sampled
sounds are transformed to fit the actual loop length of the
JamTomorrowPlayer. This length is defined using the two
values bpm (beats per minute, reflects the tempo of the mu-
sical piece) and bpl (beats per loop, defines the length of a
loop in beats). LoopSamples are played in a loop cycle. At
the beginning of every cycle, all playing LoopSamples are
stopped. The LoopPlayer then decides for each sample if it
should be played or not, based on the sample’s probability.
A JamTomorrowPlayer may hold a variety of different
LoopSample types. We propose four kinds of samples, the
trivial SimpleSample and TiledSample, and the more com-
plicated StretchedSample (not yet implemented) and Frag-
mentedSample. Each sample type needs different specific
parameters to be defined by the users.
MusinumTracks have no unified length. They simply
start to play when the player starts and continue playing un-
til either they reach their end or the player stops (see [8]
for more information about Musinum). Due to the incom-
patibility of MIDI and Croquet, we have integrated Mus-
inum into Jam Tomorrow using the interface of Squeak’s
FMSynth. Each played note is generated live with the FM-
Synth. If we have many tracks, this implementation be-
comes rather slow, but it is the only way to integrate a soft-
ware like Musinum.
</bodyText>
<subsubsectionHeader confidence="0.910168">
4.4.1 Random Numbers in Croquet
</subsubsectionHeader>
<bodyText confidence="0.999481117647059">
For the fragmentation of FragmentedSamples, we use ran-
dom numbers. This makes them sound very interesting.
But the use of random numbers causes problems in Cro-
quet. Usually, if we send a message to a random number
generator, this message is replicated on each computer. We
get a different number on each computer and the images are
not synchronized anymore. This problem is discussed on
site [10].
We have come up with another solution, made possible
by Jam Tomorrow’s architecture. The user interfaces on the
different computers are separated from each other and up-
date themselves following their models, as described above.
Like this, we can let one model calculate a random number,
which then updates the corresponding models on the other
computers with the result (by using the #meta message).
Then, the function processing the random number is called
from the first computer (also with #meta).
</bodyText>
<subsubsectionHeader confidence="0.982702">
4.4.2 Sound Cosmetics
</subsubsectionHeader>
<bodyText confidence="0.999421285714286">
Using procedures like the one in FragmentedSample, where
sound files are cut in smaller segments and reorganized,
a crackling noise appears during playback. A common
technique to prevent such noise used by professional audio
recording software like Logic [9], is crossfading. We imple-
mented a simple fading algorithm applied to each fragment
moved to the sound buffer to smoothen the generated music.
</bodyText>
<subsectionHeader confidence="0.835058">
4.5. A Croquet GUI
</subsectionHeader>
<bodyText confidence="0.989302222222222">
The actual GUI design is similar to many interfaces of
music production softwares (see Fig. 4). There is an area
with control buttons, where users can change global musical
settings, add tracks and play or stop the music. Below these
buttons, each added track is represented by a row. Such a
row holds text fields, showing the settings of the concerned
track, and different buttons, to change the track settings and
to remove the track. For each track type, there is a different
kind of row, providing specific controls.
</bodyText>
<figureCaption confidence="0.961953">
Figure 4. The GUI Morph.
</figureCaption>
<bodyText confidence="0.961911875">
It is easy to integrate a Squeak project or morph into a
Croquet space by using TWindows (see Fig. 5) in combi-
nation with TMorphics and TMorphMonitors. Any Squeak
morph can be shown in such a window, the size, orientation
and position of which users can change directly within the
space. If we attach an OpenAL sound source to the win-
dow adn the window is moved, the source is automatically
moved with it.
</bodyText>
<figureCaption confidence="0.635476">
Figure 5. An avatar in front of two GUI TWin-
dows.
</figureCaption>
<bodyText confidence="0.9978411">
Usually, all events reaching this window on one machine
are caught by its TMorphic and replicated on the other ma-
chines using #meta sends. To follow our architecture con-
cept, we had to disable this automatic replication, so that
the events just reach the local model. If we had not, it would
have been impossible to use a TMorphic/TMorphMonitor-
GUI in combination with the classes our model uses (for
example TSound), as these classes are using the #meta mes-
sage themselves.
The #addTrack method in JamTomorrowPlayer, for in-
stance, uses meta for guaranteeing that the tracks, generated
at the same time in the corresponding models of the net-
work, are seen as corresponding tracks. When this method
is linked to a button in a TWindow-GUI and the button
is pressed, on each participating machine the #addTrack
method containing the #meta send is executed. Because
of the double use of meta, the final message (#addTrack:
aTrack) is called one time per participant on each machine.
So each participant gets multiple new tracks, instead of one.
In our architecture with the separated GUIs, when a user
presses a GUI button, the #addTrack method is called just
on his machine, and only then, controlled by the model us-
ing meta, the other participant’s players get new tracks.
Our windowed GUI is not ideal for practical use in Cro-
quet. The icons are too small for being controlled from a
certain distance. For a next iteration, we could imagine to
design a more simple and intuitive GUI, showing the pa-
rameters in a more graphical or even three-dimensional way
(eventually inspired by real world interfaces as shown on
site [5]).
</bodyText>
<subsectionHeader confidence="0.628005">
4.6. Sound File Location
</subsectionHeader>
<bodyText confidence="0.9987424375">
As usual in Croquet, all users have an image with the
same code. In our actual version, this also applies to sound
files. Every user needs to have exactly the same sound files
located in a specified directory. If this is not the case, the
generated music will sound different on each computer.
Sound buffer transfer between Croquet images is very
slow at the time. We are having problems with TeaRefer-
enceStreams, as soon as we try to exchange large buffers or
many small buffers at the same time. So we have to send the
sound buffers bytewise, which takes a lot of time (several
minutes for a sound buffer of one second length). However,
using compressed audio (as used successfully in [14]) and
an improved Croquet communication system, there could
be a chance to let different users come up with their own
sound files and distribute them to their friends. Of course,
the application would be much more interesting like this.
</bodyText>
<sectionHeader confidence="0.992637" genericHeader="method">
5. Future Work
</sectionHeader>
<bodyText confidence="0.999174583333334">
It is difficult to work with technologies that are still being
built. We have now explored the actual musical possibilities
of Croquet and we are ready for refining the integrated fea-
tures as well as for implementing new ones.
For example, we thought of the possibility to define re-
lations between other objects in the world and generated
music. A moving object could play tune and users could
detect this object by following this tune. A special kind
of UI could even allow users to change this object’s tune
within the world.
The software we propose could also be used in educa-
tional domains. Children could learn to notice and locate
musical events. We could then implement a more simple,
symbolic and intuitive interface (not necessarily being con-
tained by a window) and integrate educational musical con-
cepts.
Or, as another application example, we could imagine a
virtual band as in [14], with the difference that its sound
would be three-dimensional. This means that every instru-
ment would be located at a position within the Croquet
space. Listeners could move around between these instru-
ments as if they were on stage at a virtual concert. However,
this idea cannot be realized using the current implementa-
tions of Croquet and OpenAL.
</bodyText>
<sectionHeader confidence="0.996748" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.987634222222222">
In this paper, we have presented our ideas and experi-
ences creating a next generation music software. The cur-
rent version can be downloaded from: [7]. There may be
no professional use for such a software, but in our time,
open source online collaboration becomes more and more
reputed among home users around the world.
Acknowledgements. We gratefully acknowledge the fi-
nancial support of the Swiss National Science Foundation
for the projects “A Unified Approach to Composition and
</bodyText>
<reference confidence="0.6966725">
Extensibility” (SNF Project No. 200020-105091/1, Oct.
2004 - Sept. 2006)
</reference>
<sectionHeader confidence="0.657519" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999315571428571">
[1] Digital musician: Making music online. http://-
digitalmusician.net/.
[2] C. Dobrian. Digital audio. http://music.arts.uci.edu/-
dobrian/digitalaudio.htm.
[3] ejamming, connect. create. collaborate. jam. no matter
where you are. http://www.ejamming.com/.
[4] M. Gaelli. Musinum. http://www.squeaksource.com/.
[5] Tangible user interfaces. http://www.iua.upf.es/mtg/-
reacTable/?related.
[6] G. Hiebert. Creative OpenAL Programmer’s Reference,
2001.
[7] Jam tomorrow: A collaborative composition software for
croquet. http://kilana.unibe.ch:8888/JamTomorrow/.
[8] L. Kindermann. Musinum. http://reglos.de/musinum/.
[9] Logic pro. http://www.apple.com/logicpro/.
[10] J. Lombardi. Random acts. http://jlombardi.blogspot.com/-
2004/12/random-acts.html.
[11] Model-view-controller. http://en.wikipedia.org/wiki/Model-
view-controller/.
[12] My virtual band, open source music and online collabora-
tion. http://www.myvirtualband.com/.
[13] Craig Latta: A musical collaboration network for the inter-
net. http://www.netjam.org.
[14] Ninjam, realtime music collaboration software.
http://www.ninjam.com/.
[15] O. W. Sergi Jord. A system for collaborative music com-
position over the web. In Proceedings of the 2001 Interna-
tional Computer Music Conference, La Habana, 2001. Inter-
national Computer Music Association.
[16] D. A. Smith, A. Kay, A. Raab, and D. P. Reed. Croquet, A
Collaboration System Architecture, 2003. in: Proceedings
of the First Conference on Creating, Connecting and Collab-
orating through Computing.
[17] Timidity: Midi to wave converter. http://timidity-
.sourceforge.net/.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.592721">
<title confidence="0.999649">Jam Tomorrow: Collaborative Music Generation in Croquet Using OpenAL</title>
<author confidence="0.999399">Florian Thalmann</author>
<email confidence="0.918127">thalmann@students.unibe.ch</email>
<author confidence="0.999872">Markus Gaelli</author>
<email confidence="0.764984">gaelli@iam.unibe.ch</email>
<affiliation confidence="0.99956">Institute of Computer Science and Applied Mathematics, University of Bern</affiliation>
<address confidence="0.995622">Neubrückstrasse 10, CH-3012 Bern, Switzerland</address>
<abstract confidence="0.9834958">1 We propose a music generation software that allows large numbers of users to collaborate. In a virtual world, groups of users generate music simultaneously at different places in a room. This can be realized using OpenAL sound sources. The generated musical pieces have to be modifiable while they are playing and all collaborating users should immediately see and hear the results of such modifications. We are testing these concepts within Croquet by implementing a software called Jam Tomorrow.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Extensibility”</author>
</authors>
<date>2000</date>
<marker>Extensibility”, 2000</marker>
<rawString> Extensibility” (SNF Project No. 200020-105091/1, Oct. 2004 - Sept. 2006)</rawString>
</citation>
<citation valid="false">
<title>Digital musician: Making music online.</title>
<note>http://-digitalmusician.net/.</note>
<contexts>
<context position="1804" citStr="[1]" startWordPosition="278" endWordPosition="278">fully confusing.” Lewis Carroll, Through the Looking Glass, 1871. Even though the idea of collaborative music generation on computer networks is almost thirty years old, today there are only a few projects in this domain [15]. Improvisation over the web has always been technically constrained, due to traffic limitations and synchronicity problems, but is now possible using MIDI [3] and even using compressed audio [14]. There are also projects about collaborative com1Fourth International Conference on Creating, Connecting and Collaborating through Computing (C5’06) pages 73–78 posing [15] [13] [1] or collaborative open source recording [12]. People using such compositional or improvisational software are restricted to working on one musical piece at a time. Our idea is to develop a software that lets users collaborate in generating multiple musical pieces simultaneously within a modern virtual world. People using this software will have compositional and improvisational possibilities. In this paper, we start by presenting our conceptual ideas. In section 3 we explain the technology we use for our explorative project, Jam Tomorrow. We continue in section 4 by giving you a more detailed </context>
</contexts>
<marker>[1]</marker>
<rawString>Digital musician: Making music online. http://-digitalmusician.net/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Dobrian</author>
</authors>
<note>Digital audio. http://music.arts.uci.edu/-dobrian/digitalaudio.htm.</note>
<contexts>
<context position="4354" citStr="[2]" startWordPosition="710" endWordPosition="710">ew one. It is certainly very interesting to walk through this world and hear the editors play the music reflecting the common likes of the users. 3. Technology: Croquet Project and OpenAL OpenAL is an API that “allows a programmer to position audio sources in a three-dimensional space around a listener, producing reasonable fading and panning for each source so that the environment seems threedimensional” [6]. This is exactly what we need for the placement of our musical pieces within the room. Unfortunately, at the time, OpenAL does not support MIDI yet, so we are restricted to sampled sound [2]. “Croquet is a combination of open source computer software and network architecture that supports deep collaboration and resource sharing among large numbers of users” [16] and it is implemented in Squeak. Although Croquet is still being developed, it seems to be the perfect framework for the integration of our application. A Croquet environment is divided into spaces, in which users can move around using their avatar. Unlike in clientserver applications, each participant in a Croquet network session, has an independent image holding the entire application on his computer. At the moment, for</context>
</contexts>
<marker>[2]</marker>
<rawString>C. Dobrian. Digital audio. http://music.arts.uci.edu/-dobrian/digitalaudio.htm.</rawString>
</citation>
<citation valid="false">
<authors>
<author>create collaborate jam</author>
</authors>
<note>no matter where you are. http://www.ejamming.com/.</note>
<contexts>
<context position="1585" citStr="[3]" startWordPosition="246" endWordPosition="246">never jam today.” “It must come sometimes to ‘jam today,”’Alice objected. “No it can’t,” said the Queen. “It’s jam every other day; today isn’t any other day, you know.” “I don’t understand you,” said Alice. “It’s dreadfully confusing.” Lewis Carroll, Through the Looking Glass, 1871. Even though the idea of collaborative music generation on computer networks is almost thirty years old, today there are only a few projects in this domain [15]. Improvisation over the web has always been technically constrained, due to traffic limitations and synchronicity problems, but is now possible using MIDI [3] and even using compressed audio [14]. There are also projects about collaborative com1Fourth International Conference on Creating, Connecting and Collaborating through Computing (C5’06) pages 73–78 posing [15] [13] [1] or collaborative open source recording [12]. People using such compositional or improvisational software are restricted to working on one musical piece at a time. Our idea is to develop a software that lets users collaborate in generating multiple musical pieces simultaneously within a modern virtual world. People using this software will have compositional and improvisational </context>
</contexts>
<marker>[3]</marker>
<rawString>ejamming, connect. create. collaborate. jam. no matter where you are. http://www.ejamming.com/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Gaelli</author>
</authors>
<note>Musinum. http://www.squeaksource.com/.</note>
<contexts>
<context position="11959" citStr="[4]" startWordPosition="2013" endWordPosition="2013">aying (looping), even while they are being edited. So, all modifications can be audible immediately, because there are no long formal structures to wait for. • The second form is total improvisation. Users participate live using a microphone (or a MIDI interface). However, with the current Croquet implementation, it is impossible to integrate an improvisational function for microphone or MIDI interfaces, due to synchronicity problems. Jam Tomorrow is based on the loop form, although we extended it for experimental reasons. We also integrated the possibility to generate Musinum tracks. Musinum [4] [8] is an algorithmic music generation software based on fractals. See section 4.4 for information about the implementation of our musical concept. 4.3. The Jam Tomorrow Architecture During development, we encountered several problems, mainly with the Croquet concept. Our goal was to use unmodified Croquet and Squeak parts, but we found out that what we were planning to implement was not possible without changing some methods in the Croquet framework. What we have now, is an architecture that can be seen as an application of the MVC (Model-View-Controller) pattern [11] in Croquet. We have a m</context>
</contexts>
<marker>[4]</marker>
<rawString>M. Gaelli. Musinum. http://www.squeaksource.com/.</rawString>
</citation>
<citation valid="false">
<title>Tangible user interfaces.</title>
<note>http://www.iua.upf.es/mtg/-reacTable/?related.</note>
<contexts>
<context position="20346" citStr="[5]" startWordPosition="3410" endWordPosition="3410">acks, instead of one. In our architecture with the separated GUIs, when a user presses a GUI button, the #addTrack method is called just on his machine, and only then, controlled by the model using meta, the other participant’s players get new tracks. Our windowed GUI is not ideal for practical use in Croquet. The icons are too small for being controlled from a certain distance. For a next iteration, we could imagine to design a more simple and intuitive GUI, showing the parameters in a more graphical or even three-dimensional way (eventually inspired by real world interfaces as shown on site [5]). 4.6. Sound File Location As usual in Croquet, all users have an image with the same code. In our actual version, this also applies to sound files. Every user needs to have exactly the same sound files located in a specified directory. If this is not the case, the generated music will sound different on each computer. Sound buffer transfer between Croquet images is very slow at the time. We are having problems with TeaReferenceStreams, as soon as we try to exchange large buffers or many small buffers at the same time. So we have to send the sound buffers bytewise, which takes a lot of time (</context>
</contexts>
<marker>[5]</marker>
<rawString>Tangible user interfaces. http://www.iua.upf.es/mtg/-reacTable/?related.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hiebert</author>
</authors>
<title>Creative OpenAL Programmer’s Reference,</title>
<date>2001</date>
<contexts>
<context position="4163" citStr="[6]" startWordPosition="675" endWordPosition="675">section 4.2. With our concept, we can have a large number of tunes in one huge musical world. Users in this world can collaborate in groups or work alone, modify an existing tune or start a new one. It is certainly very interesting to walk through this world and hear the editors play the music reflecting the common likes of the users. 3. Technology: Croquet Project and OpenAL OpenAL is an API that “allows a programmer to position audio sources in a three-dimensional space around a listener, producing reasonable fading and panning for each source so that the environment seems threedimensional” [6]. This is exactly what we need for the placement of our musical pieces within the room. Unfortunately, at the time, OpenAL does not support MIDI yet, so we are restricted to sampled sound [2]. “Croquet is a combination of open source computer software and network architecture that supports deep collaboration and resource sharing among large numbers of users” [16] and it is implemented in Squeak. Although Croquet is still being developed, it seems to be the perfect framework for the integration of our application. A Croquet environment is divided into spaces, in which users can move around usin</context>
</contexts>
<marker>[6]</marker>
<rawString>G. Hiebert. Creative OpenAL Programmer’s Reference, 2001.</rawString>
</citation>
<citation valid="false">
<title>Jam tomorrow: A collaborative composition software for croquet.</title>
<note>http://kilana.unibe.ch:8888/JamTomorrow/.</note>
<marker>[7]</marker>
<rawString>Jam tomorrow: A collaborative composition software for croquet. http://kilana.unibe.ch:8888/JamTomorrow/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Kindermann</author>
</authors>
<note>Musinum. http://reglos.de/musinum/.</note>
<contexts>
<context position="11963" citStr="[8]" startWordPosition="2014" endWordPosition="2014">g (looping), even while they are being edited. So, all modifications can be audible immediately, because there are no long formal structures to wait for. • The second form is total improvisation. Users participate live using a microphone (or a MIDI interface). However, with the current Croquet implementation, it is impossible to integrate an improvisational function for microphone or MIDI interfaces, due to synchronicity problems. Jam Tomorrow is based on the loop form, although we extended it for experimental reasons. We also integrated the possibility to generate Musinum tracks. Musinum [4] [8] is an algorithmic music generation software based on fractals. See section 4.4 for information about the implementation of our musical concept. 4.3. The Jam Tomorrow Architecture During development, we encountered several problems, mainly with the Croquet concept. Our goal was to use unmodified Croquet and Squeak parts, but we found out that what we were planning to implement was not possible without changing some methods in the Croquet framework. What we have now, is an architecture that can be seen as an application of the MVC (Model-View-Controller) pattern [11] in Croquet. We have a model</context>
<context position="15999" citStr="[8]" startWordPosition="2672" endWordPosition="2672">e stopped. The LoopPlayer then decides for each sample if it should be played or not, based on the sample’s probability. A JamTomorrowPlayer may hold a variety of different LoopSample types. We propose four kinds of samples, the trivial SimpleSample and TiledSample, and the more complicated StretchedSample (not yet implemented) and FragmentedSample. Each sample type needs different specific parameters to be defined by the users. MusinumTracks have no unified length. They simply start to play when the player starts and continue playing until either they reach their end or the player stops (see [8] for more information about Musinum). Due to the incompatibility of MIDI and Croquet, we have integrated Musinum into Jam Tomorrow using the interface of Squeak’s FMSynth. Each played note is generated live with the FMSynth. If we have many tracks, this implementation becomes rather slow, but it is the only way to integrate a software like Musinum. 4.4.1 Random Numbers in Croquet For the fragmentation of FragmentedSamples, we use random numbers. This makes them sound very interesting. But the use of random numbers causes problems in Croquet. Usually, if we send a message to a random number gen</context>
</contexts>
<marker>[8]</marker>
<rawString>L. Kindermann. Musinum. http://reglos.de/musinum/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Logic pro</author>
</authors>
<note>http://www.apple.com/logicpro/.</note>
<contexts>
<context position="17561" citStr="[9]" startWordPosition="2928" endWordPosition="2928">elves following their models, as described above. Like this, we can let one model calculate a random number, which then updates the corresponding models on the other computers with the result (by using the #meta message). Then, the function processing the random number is called from the first computer (also with #meta). 4.4.2 Sound Cosmetics Using procedures like the one in FragmentedSample, where sound files are cut in smaller segments and reorganized, a crackling noise appears during playback. A common technique to prevent such noise used by professional audio recording software like Logic [9], is crossfading. We implemented a simple fading algorithm applied to each fragment moved to the sound buffer to smoothen the generated music. 4.5. A Croquet GUI The actual GUI design is similar to many interfaces of music production softwares (see Fig. 4). There is an area with control buttons, where users can change global musical settings, add tracks and play or stop the music. Below these buttons, each added track is represented by a row. Such a row holds text fields, showing the settings of the concerned track, and different buttons, to change the track settings and to remove the track. F</context>
</contexts>
<marker>[9]</marker>
<rawString>Logic pro. http://www.apple.com/logicpro/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Lombardi</author>
</authors>
<title>Random acts.</title>
<note>http://jlombardi.blogspot.com/-2004/12/random-acts.html.</note>
<contexts>
<context position="16778" citStr="[10]" startWordPosition="2806" endWordPosition="2806">h played note is generated live with the FMSynth. If we have many tracks, this implementation becomes rather slow, but it is the only way to integrate a software like Musinum. 4.4.1 Random Numbers in Croquet For the fragmentation of FragmentedSamples, we use random numbers. This makes them sound very interesting. But the use of random numbers causes problems in Croquet. Usually, if we send a message to a random number generator, this message is replicated on each computer. We get a different number on each computer and the images are not synchronized anymore. This problem is discussed on site [10]. We have come up with another solution, made possible by Jam Tomorrow’s architecture. The user interfaces on the different computers are separated from each other and update themselves following their models, as described above. Like this, we can let one model calculate a random number, which then updates the corresponding models on the other computers with the result (by using the #meta message). Then, the function processing the random number is called from the first computer (also with #meta). 4.4.2 Sound Cosmetics Using procedures like the one in FragmentedSample, where sound files are cu</context>
</contexts>
<marker>[10]</marker>
<rawString>J. Lombardi. Random acts. http://jlombardi.blogspot.com/-2004/12/random-acts.html.</rawString>
</citation>
<citation valid="false">
<note>Model-view-controller. http://en.wikipedia.org/wiki/Modelview-controller/.</note>
<contexts>
<context position="12535" citStr="[11]" startWordPosition="2106" endWordPosition="2106"> Musinum tracks. Musinum [4] [8] is an algorithmic music generation software based on fractals. See section 4.4 for information about the implementation of our musical concept. 4.3. The Jam Tomorrow Architecture During development, we encountered several problems, mainly with the Croquet concept. Our goal was to use unmodified Croquet and Squeak parts, but we found out that what we were planning to implement was not possible without changing some methods in the Croquet framework. What we have now, is an architecture that can be seen as an application of the MVC (Model-View-Controller) pattern [11] in Croquet. We have a model consisting of the player and its tracks. It is the responsibility of this model to keep the corresponding models of the other images in the network synchronized (see Fig. 2). Each model has one view by default, which is updated automatically. The views in the different images only know their own models, so they are separated from each other. This architecture with separated views does not fit the Croquet concept perfectly. However, this is not noticeable for users, because the views stay synchronized through their model 2 model 1 model 3 editor 1 event synchronizat</context>
</contexts>
<marker>[11]</marker>
<rawString>Model-view-controller. http://en.wikipedia.org/wiki/Modelview-controller/.</rawString>
</citation>
<citation valid="false">
<title>My virtual band, open source music and online collaboration.</title>
<note>http://www.myvirtualband.com/.</note>
<contexts>
<context position="1848" citStr="[12]" startWordPosition="285" endWordPosition="285">e Looking Glass, 1871. Even though the idea of collaborative music generation on computer networks is almost thirty years old, today there are only a few projects in this domain [15]. Improvisation over the web has always been technically constrained, due to traffic limitations and synchronicity problems, but is now possible using MIDI [3] and even using compressed audio [14]. There are also projects about collaborative com1Fourth International Conference on Creating, Connecting and Collaborating through Computing (C5’06) pages 73–78 posing [15] [13] [1] or collaborative open source recording [12]. People using such compositional or improvisational software are restricted to working on one musical piece at a time. Our idea is to develop a software that lets users collaborate in generating multiple musical pieces simultaneously within a modern virtual world. People using this software will have compositional and improvisational possibilities. In this paper, we start by presenting our conceptual ideas. In section 3 we explain the technology we use for our explorative project, Jam Tomorrow. We continue in section 4 by giving you a more detailed description of the musical concept and the a</context>
</contexts>
<marker>[12]</marker>
<rawString>My virtual band, open source music and online collaboration. http://www.myvirtualband.com/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Craig Latta</author>
</authors>
<title>A musical collaboration network for the internet.</title>
<location>http://www.netjam.org.</location>
<contexts>
<context position="1800" citStr="[13]" startWordPosition="277" endWordPosition="277">dreadfully confusing.” Lewis Carroll, Through the Looking Glass, 1871. Even though the idea of collaborative music generation on computer networks is almost thirty years old, today there are only a few projects in this domain [15]. Improvisation over the web has always been technically constrained, due to traffic limitations and synchronicity problems, but is now possible using MIDI [3] and even using compressed audio [14]. There are also projects about collaborative com1Fourth International Conference on Creating, Connecting and Collaborating through Computing (C5’06) pages 73–78 posing [15] [13] [1] or collaborative open source recording [12]. People using such compositional or improvisational software are restricted to working on one musical piece at a time. Our idea is to develop a software that lets users collaborate in generating multiple musical pieces simultaneously within a modern virtual world. People using this software will have compositional and improvisational possibilities. In this paper, we start by presenting our conceptual ideas. In section 3 we explain the technology we use for our explorative project, Jam Tomorrow. We continue in section 4 by giving you a more detai</context>
</contexts>
<marker>[13]</marker>
<rawString>Craig Latta: A musical collaboration network for the internet. http://www.netjam.org.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ninjam</author>
</authors>
<title>realtime music collaboration software.</title>
<note>http://www.ninjam.com/.</note>
<contexts>
<context position="1622" citStr="[14]" startWordPosition="253" endWordPosition="253">times to ‘jam today,”’Alice objected. “No it can’t,” said the Queen. “It’s jam every other day; today isn’t any other day, you know.” “I don’t understand you,” said Alice. “It’s dreadfully confusing.” Lewis Carroll, Through the Looking Glass, 1871. Even though the idea of collaborative music generation on computer networks is almost thirty years old, today there are only a few projects in this domain [15]. Improvisation over the web has always been technically constrained, due to traffic limitations and synchronicity problems, but is now possible using MIDI [3] and even using compressed audio [14]. There are also projects about collaborative com1Fourth International Conference on Creating, Connecting and Collaborating through Computing (C5’06) pages 73–78 posing [15] [13] [1] or collaborative open source recording [12]. People using such compositional or improvisational software are restricted to working on one musical piece at a time. Our idea is to develop a software that lets users collaborate in generating multiple musical pieces simultaneously within a modern virtual world. People using this software will have compositional and improvisational possibilities. In this paper, we star</context>
<context position="21065" citStr="[14]" startWordPosition="3537" endWordPosition="3537"> this also applies to sound files. Every user needs to have exactly the same sound files located in a specified directory. If this is not the case, the generated music will sound different on each computer. Sound buffer transfer between Croquet images is very slow at the time. We are having problems with TeaReferenceStreams, as soon as we try to exchange large buffers or many small buffers at the same time. So we have to send the sound buffers bytewise, which takes a lot of time (several minutes for a sound buffer of one second length). However, using compressed audio (as used successfully in [14]) and an improved Croquet communication system, there could be a chance to let different users come up with their own sound files and distribute them to their friends. Of course, the application would be much more interesting like this. 5. Future Work It is difficult to work with technologies that are still being built. We have now explored the actual musical possibilities of Croquet and we are ready for refining the integrated features as well as for implementing new ones. For example, we thought of the possibility to define relations between other objects in the world and generated music. A </context>
</contexts>
<marker>[14]</marker>
<rawString>Ninjam, realtime music collaboration software. http://www.ninjam.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O W Sergi Jord</author>
</authors>
<title>A system for collaborative music composition over the web.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 International Computer Music Conference, La Habana,</booktitle>
<institution>International Computer Music Association.</institution>
<contexts>
<context position="1426" citStr="[15]" startWordPosition="223" endWordPosition="223"> Queen. “Well, I don’t want any today, at any rate.” “You couldn’t have it if you did want it,” the Queen said. “The rule is jam tomorrow and jam yesterday but never jam today.” “It must come sometimes to ‘jam today,”’Alice objected. “No it can’t,” said the Queen. “It’s jam every other day; today isn’t any other day, you know.” “I don’t understand you,” said Alice. “It’s dreadfully confusing.” Lewis Carroll, Through the Looking Glass, 1871. Even though the idea of collaborative music generation on computer networks is almost thirty years old, today there are only a few projects in this domain [15]. Improvisation over the web has always been technically constrained, due to traffic limitations and synchronicity problems, but is now possible using MIDI [3] and even using compressed audio [14]. There are also projects about collaborative com1Fourth International Conference on Creating, Connecting and Collaborating through Computing (C5’06) pages 73–78 posing [15] [13] [1] or collaborative open source recording [12]. People using such compositional or improvisational software are restricted to working on one musical piece at a time. Our idea is to develop a software that lets users collabor</context>
</contexts>
<marker>[15]</marker>
<rawString>O. W. Sergi Jord. A system for collaborative music composition over the web. In Proceedings of the 2001 International Computer Music Conference, La Habana, 2001. International Computer Music Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Croquet</author>
</authors>
<title>A Collaboration System Architecture,</title>
<date>2003</date>
<booktitle>Proceedings of the First Conference on Creating, Connecting and Collaborating through Computing.</booktitle>
<contexts>
<context position="4528" citStr="[16]" startWordPosition="736" endWordPosition="736">ect and OpenAL OpenAL is an API that “allows a programmer to position audio sources in a three-dimensional space around a listener, producing reasonable fading and panning for each source so that the environment seems threedimensional” [6]. This is exactly what we need for the placement of our musical pieces within the room. Unfortunately, at the time, OpenAL does not support MIDI yet, so we are restricted to sampled sound [2]. “Croquet is a combination of open source computer software and network architecture that supports deep collaboration and resource sharing among large numbers of users” [16] and it is implemented in Squeak. Although Croquet is still being developed, it seems to be the perfect framework for the integration of our application. A Croquet environment is divided into spaces, in which users can move around using their avatar. Unlike in clientserver applications, each participant in a Croquet network session, has an independent image holding the entire application on his computer. At the moment, for establishing a connection between Croquet users, all images have to be in the same state and contain the same program code. To keep the images synchronized, every message se</context>
</contexts>
<marker>[16]</marker>
<rawString>D. A. Smith, A. Kay, A. Raab, and D. P. Reed. Croquet, A Collaboration System Architecture, 2003. in: Proceedings of the First Conference on Creating, Connecting and Collaborating through Computing.</rawString>
</citation>
<citation valid="false">
<note>Timidity: Midi to wave converter. http://timidity.sourceforge.net/.</note>
<contexts>
<context position="7783" citStr="[17]" startWordPosition="1283" endWordPosition="1283">lly saved or generated sounds. Improvisation over network using microphones is therefore not yet possible. To integrate realtime MIDI functions using TSounds would result in an even more unsatisfying performance. We have experimented with Squeak’s FMSynth, but the translation to TSounds is too slow for complex MIDI messages. However, it could be possible to build a simple MIDI application, where every note is pregenerated as TSound. We hope, that OpenAL will soon support MIDI for this would increase our musical flexibility dramatically. There are software products that translate MIDI to audio [17], but not yet in realtime. At the moment, Croquet cannot be used for virtual band playing in realtime yet. This is why we chose to build a first application that does not need to be strongly synchronous. In our concept it is important, that every participant hears exactly the samemusic, but not necessarily at the same time. For example, it is possible, that the user who presses the ’play’-button, hears the music a bit earlier than his fellow users. 4. Implementation: Jam Tomorrow For experimentation with Croquet and OpenAL, we have built a simple prototype called Jam Tomorrow. It consists of a</context>
</contexts>
<marker>[17]</marker>
<rawString>Timidity: Midi to wave converter. http://timidity.sourceforge.net/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>